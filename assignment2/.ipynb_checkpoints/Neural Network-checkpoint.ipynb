{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n3/rc0k_vzs5ln66jll2sd1t7wh0000gn/T/ipykernel_28337/2810386256.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/var/folders/n3/rc0k_vzs5ln66jll2sd1t7wh0000gn/T/ipykernel_28337/2810386256.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "# Done\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "# Done\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "# Done\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "# Done\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "# Done\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "# Done\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "# Done\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.320001, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 2, loss: 2.301891, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 3, loss: 2.301884, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 4, loss: 2.301873, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 5, loss: 2.301876, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 6, loss: 2.301879, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 7, loss: 2.301896, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 8, loss: 2.301876, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 9, loss: 2.301893, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 10, loss: 2.301887, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 11, loss: 2.301884, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 12, loss: 2.301876, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 13, loss: 2.301887, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 14, loss: 2.301884, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 15, loss: 2.301895, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 16, loss: 2.301885, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 17, loss: 2.301883, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 18, loss: 2.301898, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 19, loss: 2.301875, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 20, loss: 2.301890, train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.290610, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 2, loss: 2.269078, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 3, loss: 2.263781, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 4, loss: 2.261145, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 5, loss: 2.258856, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 6, loss: 2.257002, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 7, loss: 2.255838, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 8, loss: 2.255067, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 9, loss: 2.254631, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 10, loss: 2.254321, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 11, loss: 2.254145, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 12, loss: 2.253836, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 13, loss: 2.253693, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 14, loss: 2.253459, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 15, loss: 2.253216, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 16, loss: 2.252938, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 17, loss: 2.252474, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 18, loss: 2.252029, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 19, loss: 2.251139, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 20, loss: 2.249937, train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.314097, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 2, loss: 2.306514, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 3, loss: 2.300148, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 4, loss: 2.294796, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 5, loss: 2.290296, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 6, loss: 2.286500, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 7, loss: 2.283282, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 8, loss: 2.280553, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 9, loss: 2.278230, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 10, loss: 2.276249, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 11, loss: 2.274549, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 12, loss: 2.273087, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 13, loss: 2.271828, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 14, loss: 2.270736, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 15, loss: 2.269783, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 16, loss: 2.268951, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 17, loss: 2.268216, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 18, loss: 2.267566, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 19, loss: 2.266988, train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch 20, loss: 2.266470, train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.321096, train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Epoch 2, loss: 2.309684, train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Epoch 3, loss: 2.298008, train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch 4, loss: 2.288348, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 5, loss: 2.281775, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 6, loss: 2.269475, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 7, loss: 2.255200, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 8, loss: 2.233219, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 9, loss: 2.176926, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 10, loss: 2.103371, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 11, loss: 2.004368, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 12, loss: 1.955721, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 13, loss: 1.884039, train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Epoch 14, loss: 1.817941, train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Epoch 15, loss: 1.765364, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 16, loss: 1.738305, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 17, loss: 1.700878, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 18, loss: 1.705722, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 19, loss: 1.674520, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 20, loss: 1.635517, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 21, loss: 1.622347, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 22, loss: 1.662648, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 23, loss: 1.638737, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 24, loss: 1.586666, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 25, loss: 1.538351, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 26, loss: 1.575403, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 27, loss: 1.524925, train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch 28, loss: 1.462931, train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch 29, loss: 1.420079, train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Epoch 30, loss: 1.400515, train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Epoch 31, loss: 1.401628, train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Epoch 32, loss: 1.379734, train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Epoch 33, loss: 1.349902, train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Epoch 34, loss: 1.342755, train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Epoch 35, loss: 1.320074, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 36, loss: 1.303293, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 37, loss: 1.279594, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 38, loss: 1.245713, train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Epoch 39, loss: 1.224734, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 40, loss: 1.215117, train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Epoch 41, loss: 1.171277, train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Epoch 42, loss: 1.152790, train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Epoch 43, loss: 1.135258, train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Epoch 44, loss: 1.125939, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 45, loss: 1.083387, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 46, loss: 1.096764, train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Epoch 47, loss: 1.080188, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 48, loss: 1.070505, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 49, loss: 1.046855, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 50, loss: 1.042262, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 51, loss: 1.029278, train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Epoch 52, loss: 1.030124, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 53, loss: 1.029708, train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Epoch 54, loss: 1.022580, train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Epoch 55, loss: 1.014311, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 56, loss: 1.017566, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 57, loss: 1.019009, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 58, loss: 1.002969, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 59, loss: 0.985643, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 60, loss: 0.993988, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 61, loss: 0.973764, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 62, loss: 0.965890, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 63, loss: 0.962155, train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Epoch 64, loss: 0.942670, train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Epoch 65, loss: 0.945814, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 66, loss: 0.918627, train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Epoch 67, loss: 0.918940, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 68, loss: 0.904670, train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Epoch 69, loss: 0.888678, train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Epoch 70, loss: 0.869975, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 71, loss: 0.862976, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 72, loss: 0.860945, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 73, loss: 0.843187, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 74, loss: 0.832747, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 75, loss: 0.829074, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 76, loss: 0.831729, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 77, loss: 0.821689, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 78, loss: 0.815404, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 79, loss: 0.816353, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 80, loss: 0.805616, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 81, loss: 0.810432, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 82, loss: 0.804380, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 83, loss: 0.799048, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 84, loss: 0.797017, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 85, loss: 0.796150, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 86, loss: 0.799377, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 87, loss: 0.799533, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 88, loss: 0.793016, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 89, loss: 0.789067, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 90, loss: 0.787186, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 91, loss: 0.783911, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 92, loss: 0.790624, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 93, loss: 0.783427, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 94, loss: 0.784334, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 95, loss: 0.780928, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 96, loss: 0.784242, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 97, loss: 0.784436, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 98, loss: 0.778206, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 99, loss: 0.781705, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 100, loss: 0.777394, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 101, loss: 0.773771, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 102, loss: 0.775954, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 103, loss: 0.775525, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 104, loss: 0.778411, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 105, loss: 0.773220, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 106, loss: 0.773264, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 107, loss: 0.770020, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 108, loss: 0.773298, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 109, loss: 0.768055, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 110, loss: 0.771201, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 111, loss: 0.770686, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 112, loss: 0.769185, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 113, loss: 0.771653, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 114, loss: 0.772371, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 115, loss: 0.767625, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 116, loss: 0.768257, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 117, loss: 0.767901, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 118, loss: 0.766014, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 119, loss: 0.762432, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 120, loss: 0.767863, train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121, loss: 0.763296, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 122, loss: 0.763023, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 123, loss: 0.770353, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 124, loss: 0.761338, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 125, loss: 0.762737, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 126, loss: 0.765124, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 127, loss: 0.761682, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 128, loss: 0.762443, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 129, loss: 0.763601, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 130, loss: 0.760982, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 131, loss: 0.758997, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 132, loss: 0.756451, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 133, loss: 0.761690, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 134, loss: 0.764332, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 135, loss: 0.760183, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 136, loss: 0.758673, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 137, loss: 0.766581, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 138, loss: 0.760220, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 139, loss: 0.757169, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 140, loss: 0.760920, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 141, loss: 0.757238, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 142, loss: 0.758211, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 143, loss: 0.756542, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 144, loss: 0.758994, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 145, loss: 0.757403, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 146, loss: 0.755152, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 147, loss: 0.755910, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 148, loss: 0.759213, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 149, loss: 0.756542, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 150, loss: 0.760760, train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.309255, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 2, loss: 2.280932, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 3, loss: 2.222795, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 4, loss: 2.137591, train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Epoch 5, loss: 1.955319, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 6, loss: 1.893873, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 7, loss: 1.801608, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 8, loss: 1.505319, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 9, loss: 1.742317, train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch 10, loss: 1.222721, train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Epoch 11, loss: 1.065995, train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Epoch 12, loss: 0.757859, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 13, loss: 0.597623, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 14, loss: 0.306036, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 15, loss: 0.266402, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 16, loss: 0.276980, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 17, loss: 0.282811, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 18, loss: 0.289997, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 19, loss: 0.291600, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 20, loss: 0.288924, train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-2)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "# Done\n",
    "trainer = Trainer(model, dataset, MomentumSGD(momentum=0.9), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with hidden layer size 64, reg strength 0.010000, and learning rate 0.100000\n",
      "Epoch 1, loss: 2.308749, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 2, loss: 2.274842, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 3, loss: 2.249555, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 4, loss: 2.150095, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 5, loss: 2.059108, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 6, loss: 1.707764, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 7, loss: 1.899362, train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Epoch 8, loss: 1.835277, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 9, loss: 1.716166, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 10, loss: 2.796849, train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Epoch 11, loss: 2.503142, train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Epoch 12, loss: 0.964378, train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Epoch 13, loss: 0.881310, train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Epoch 14, loss: 2.226331, train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Epoch 15, loss: 1.106018, train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Epoch 16, loss: 1.199411, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Epoch 17, loss: 0.992763, train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Epoch 18, loss: 3.914950, train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Epoch 19, loss: 3.192394, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 20, loss: 1.636060, train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Validation accuracy: 0.066667\n",
      "Training model with hidden layer size 64, reg strength 0.010000, and learning rate 0.050000\n",
      "Epoch 1, loss: 2.304204, train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Epoch 2, loss: 2.290657, train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Epoch 3, loss: 2.269613, train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Epoch 4, loss: 2.237011, train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Epoch 5, loss: 2.202137, train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Epoch 6, loss: 2.124961, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 7, loss: 1.979571, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 8, loss: 1.786904, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 9, loss: 1.730899, train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch 10, loss: 1.824011, train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Epoch 11, loss: 1.533326, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 12, loss: 1.437890, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 13, loss: 1.319922, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 14, loss: 1.169958, train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Epoch 15, loss: 0.974599, train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Epoch 16, loss: 0.787559, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 17, loss: 0.669850, train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Epoch 18, loss: 0.594914, train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Epoch 19, loss: 0.514632, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Epoch 20, loss: 0.412383, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Validation accuracy: 0.000000\n",
      "Training model with hidden layer size 64, reg strength 0.100000, and learning rate 0.100000\n",
      "Epoch 1, loss: 2.316620, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 2, loss: 2.290790, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 3, loss: 2.255476, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 4, loss: 2.205548, train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Epoch 5, loss: 2.213091, train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch 6, loss: 1.920012, train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch 7, loss: 1.980284, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 8, loss: 1.820828, train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Epoch 9, loss: 1.765561, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 10, loss: 1.565002, train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Epoch 11, loss: 1.492664, train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Epoch 12, loss: 1.494492, train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Epoch 13, loss: 1.306180, train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Epoch 14, loss: 1.220718, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 15, loss: 1.149373, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 16, loss: 1.104508, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 17, loss: 0.981245, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 18, loss: 0.901398, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 19, loss: 0.910413, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 20, loss: 0.862125, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Validation accuracy: 0.000000\n",
      "Training model with hidden layer size 64, reg strength 0.100000, and learning rate 0.050000\n",
      "Epoch 1, loss: 2.313384, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 2, loss: 2.297419, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 3, loss: 2.273808, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 4, loss: 2.255394, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 5, loss: 2.230968, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 6, loss: 2.174135, train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch 7, loss: 2.033528, train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch 8, loss: 1.875793, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 9, loss: 1.755474, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 10, loss: 1.722723, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 11, loss: 1.635475, train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Epoch 12, loss: 1.660744, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 13, loss: 1.556563, train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Epoch 14, loss: 1.480739, train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Epoch 15, loss: 1.413877, train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Epoch 16, loss: 1.324087, train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Epoch 17, loss: 1.271309, train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Epoch 18, loss: 1.223612, train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Epoch 19, loss: 1.160384, train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Epoch 20, loss: 1.115920, train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Validation accuracy: 0.066667\n",
      "Training model with hidden layer size 128, reg strength 0.010000, and learning rate 0.100000\n",
      "Epoch 1, loss: 2.308036, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 2, loss: 2.281793, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 3, loss: 2.218254, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 4, loss: 2.114028, train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch 5, loss: 1.826766, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 6, loss: 1.795513, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 7, loss: 1.725011, train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Epoch 8, loss: 1.446884, train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch 9, loss: 1.346727, train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Epoch 10, loss: 0.969892, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 11, loss: 0.706348, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 12, loss: 0.596912, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Epoch 13, loss: 0.491389, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 14, loss: 0.340050, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 15, loss: 0.285489, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 16, loss: 0.286629, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 17, loss: 0.271565, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 18, loss: 0.272906, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 19, loss: 0.262409, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 20, loss: 0.257122, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Validation accuracy: 0.000000\n",
      "Training model with hidden layer size 128, reg strength 0.010000, and learning rate 0.050000\n",
      "Epoch 1, loss: 2.304923, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 2, loss: 2.295595, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 3, loss: 2.269549, train accuracy: 0.200000, val accuracy: 0.133333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 2.231770, train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Epoch 5, loss: 2.176199, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 6, loss: 2.089047, train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch 7, loss: 1.817246, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 8, loss: 1.647119, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 9, loss: 1.598977, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 10, loss: 1.576069, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 11, loss: 1.447866, train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Epoch 12, loss: 1.467372, train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch 13, loss: 1.113292, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 14, loss: 1.094173, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 15, loss: 0.839672, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 16, loss: 0.760332, train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Epoch 17, loss: 0.669198, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Epoch 18, loss: 0.511377, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 19, loss: 0.473198, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 20, loss: 0.362256, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Validation accuracy: 0.000000\n",
      "Training model with hidden layer size 128, reg strength 0.100000, and learning rate 0.100000\n",
      "Epoch 1, loss: 2.323321, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 2, loss: 2.299600, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 3, loss: 2.262335, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 4, loss: 2.201573, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 5, loss: 2.065473, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 6, loss: 1.723644, train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Epoch 7, loss: 1.877470, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 8, loss: 1.598379, train accuracy: 0.533333, val accuracy: 0.133333\n",
      "Epoch 9, loss: 1.836491, train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Epoch 10, loss: 1.527390, train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Epoch 11, loss: 1.417692, train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Epoch 12, loss: 1.416777, train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Epoch 13, loss: 1.372441, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Epoch 14, loss: 1.284029, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Epoch 15, loss: 1.136080, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 16, loss: 1.038311, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 17, loss: 1.036643, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 18, loss: 1.292608, train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Epoch 19, loss: 1.118276, train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Epoch 20, loss: 1.403588, train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Validation accuracy: 0.000000\n",
      "Training model with hidden layer size 128, reg strength 0.100000, and learning rate 0.050000\n",
      "Epoch 1, loss: 2.321452, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 2, loss: 2.311418, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 3, loss: 2.286098, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 4, loss: 2.254878, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 5, loss: 2.220490, train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Epoch 6, loss: 2.120907, train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Epoch 7, loss: 2.011873, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 8, loss: 1.875996, train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch 9, loss: 1.685807, train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Epoch 10, loss: 1.997748, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 11, loss: 1.742705, train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Epoch 12, loss: 1.638196, train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch 13, loss: 1.567128, train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Epoch 14, loss: 1.457373, train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Epoch 15, loss: 1.362178, train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch 16, loss: 1.243365, train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Epoch 17, loss: 1.178705, train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Epoch 18, loss: 1.155858, train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Epoch 19, loss: 1.077775, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Epoch 20, loss: 1.093827, train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Validation accuracy: 0.066667\n",
      "Best validation accuracy achieved: 0.066667\n"
     ]
    }
   ],
   "source": [
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "hidden_layer_sizes = [64, 128]\n",
    "reg_strengths = [1e-2, 1e-1]\n",
    "learning_rates = [1e-1, .5e-1]\n",
    "\n",
    "for hl_size in hidden_layer_sizes:\n",
    "    for reg in reg_strengths:\n",
    "        for lr in learning_rates:\n",
    "            print('Training model with hidden layer size %d, reg strength %f, and learning rate %f' % (hl_size, reg, lr))\n",
    "            model = TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=hl_size, reg=reg)\n",
    "            dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "            trainer = Trainer(model, dataset, MomentumSGD(momentum=0.9), learning_rate=lr, num_epochs=20, batch_size=5)\n",
    "            loss_history, train_history, val_history = trainer.fit()\n",
    "            val_accuracy = val_history[-1]\n",
    "            print('Validation accuracy: %f' % val_accuracy)\n",
    "            if best_val_accuracy is None or val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_classifier = model\n",
    "                loss_history_best = loss_history\n",
    "                train_history_best = train_history\n",
    "                val_history_best = val_history\n",
    "\n",
    "print('Best validation accuracy achieved: %f' % best_val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x112ff6a30>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.122000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
